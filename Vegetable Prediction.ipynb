{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "895ab2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from keras.layers import Convolution2D ,Activation, Conv2D,MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff96b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'C:\\Users\\mayan\\Downloads\\archive (6)\\Vegetable Images\\train'\n",
    "test_dir = r'C:\\Users\\mayan\\Downloads\\archive (6)\\Vegetable Images\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9d2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,    # use for feature scaling\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)    # image transformation to avoid overfitting\n",
    "training_set = train_datagen.flow_from_directory(train_dir,   # directory path\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d781ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)   # only rescaling for test set like previous where we apply only transform\n",
    "test_set = test_datagen.flow_from_directory(test_dir,   # method to test set and fit_transform to training set\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89af7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da4e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19f117e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1aab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#keep same second convolution layer except the input_size....we dont need another input to put in the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9f56033",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd1b0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f634d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3efeadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec14bb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 38s 82ms/step - loss: 0.2312 - accuracy: 0.9279 - val_loss: 0.4478 - val_accuracy: 0.8677\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.1924 - accuracy: 0.9377 - val_loss: 0.3168 - val_accuracy: 0.9060\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.1601 - accuracy: 0.9515 - val_loss: 0.2750 - val_accuracy: 0.9120\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.1505 - accuracy: 0.9533 - val_loss: 0.1567 - val_accuracy: 0.9517\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 41s 88ms/step - loss: 0.1341 - accuracy: 0.9575 - val_loss: 0.2469 - val_accuracy: 0.9277\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.1285 - accuracy: 0.9604 - val_loss: 0.2965 - val_accuracy: 0.9117\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.1077 - accuracy: 0.9663 - val_loss: 0.1478 - val_accuracy: 0.9560\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.1036 - accuracy: 0.9659 - val_loss: 0.3462 - val_accuracy: 0.9117\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.1055 - accuracy: 0.9663 - val_loss: 0.2264 - val_accuracy: 0.9307\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.0861 - accuracy: 0.9731 - val_loss: 0.2013 - val_accuracy: 0.9400\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.0869 - accuracy: 0.9737 - val_loss: 0.1800 - val_accuracy: 0.9540\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.0835 - accuracy: 0.9738 - val_loss: 0.2711 - val_accuracy: 0.9253\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.0792 - accuracy: 0.9731 - val_loss: 0.2713 - val_accuracy: 0.9297\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 42s 90ms/step - loss: 0.0769 - accuracy: 0.9744 - val_loss: 0.3967 - val_accuracy: 0.9133\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 43s 91ms/step - loss: 0.0765 - accuracy: 0.9743 - val_loss: 0.1506 - val_accuracy: 0.9573\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 43s 93ms/step - loss: 0.0705 - accuracy: 0.9762 - val_loss: 0.2697 - val_accuracy: 0.9203\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 43s 93ms/step - loss: 0.0607 - accuracy: 0.9801 - val_loss: 0.2186 - val_accuracy: 0.9440\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 44s 93ms/step - loss: 0.0676 - accuracy: 0.9785 - val_loss: 0.3209 - val_accuracy: 0.9210\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 44s 93ms/step - loss: 0.0512 - accuracy: 0.9848 - val_loss: 0.1926 - val_accuracy: 0.9527\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0655 - accuracy: 0.9779 - val_loss: 0.3550 - val_accuracy: 0.9100\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 44s 95ms/step - loss: 0.0593 - accuracy: 0.9811 - val_loss: 0.2151 - val_accuracy: 0.9510\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0651 - accuracy: 0.9797 - val_loss: 0.3254 - val_accuracy: 0.9260\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0561 - accuracy: 0.9811 - val_loss: 0.4069 - val_accuracy: 0.9193\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0438 - accuracy: 0.9863 - val_loss: 0.4170 - val_accuracy: 0.9173\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0676 - accuracy: 0.9796 - val_loss: 0.3447 - val_accuracy: 0.9307\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 46s 99ms/step - loss: 0.0467 - accuracy: 0.9846 - val_loss: 0.2088 - val_accuracy: 0.9473\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 45s 97ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 0.3243 - val_accuracy: 0.9277\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0495 - accuracy: 0.9840 - val_loss: 0.3653 - val_accuracy: 0.9220\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 45s 97ms/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 0.1868 - val_accuracy: 0.9593\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 47s 99ms/step - loss: 0.0549 - accuracy: 0.9825 - val_loss: 0.3174 - val_accuracy: 0.9353\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0492 - accuracy: 0.9855 - val_loss: 0.4198 - val_accuracy: 0.9250\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 46s 99ms/step - loss: 0.0373 - accuracy: 0.9878 - val_loss: 0.1484 - val_accuracy: 0.9633\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 47s 100ms/step - loss: 0.0412 - accuracy: 0.9871 - val_loss: 0.1282 - val_accuracy: 0.9713\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0507 - accuracy: 0.9857 - val_loss: 0.2297 - val_accuracy: 0.9517\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.2757 - val_accuracy: 0.9440\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.3867 - val_accuracy: 0.9237\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0357 - accuracy: 0.9890 - val_loss: 0.1976 - val_accuracy: 0.9613\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 45s 97ms/step - loss: 0.0312 - accuracy: 0.9892 - val_loss: 0.1644 - val_accuracy: 0.9690\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 47s 101ms/step - loss: 0.0425 - accuracy: 0.9869 - val_loss: 0.2075 - val_accuracy: 0.9553\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0408 - accuracy: 0.9869 - val_loss: 0.3201 - val_accuracy: 0.9403\n",
      "Epoch 41/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0383 - accuracy: 0.9882 - val_loss: 0.2256 - val_accuracy: 0.9550\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.0434 - accuracy: 0.9857 - val_loss: 0.1804 - val_accuracy: 0.9633\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 44s 93ms/step - loss: 0.0378 - accuracy: 0.9879 - val_loss: 0.2883 - val_accuracy: 0.9403\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 44s 95ms/step - loss: 0.0379 - accuracy: 0.9889 - val_loss: 0.1661 - val_accuracy: 0.9647\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0293 - accuracy: 0.9909 - val_loss: 0.2015 - val_accuracy: 0.9573\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 45s 97ms/step - loss: 0.0269 - accuracy: 0.9909 - val_loss: 0.1751 - val_accuracy: 0.9727\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.0425 - accuracy: 0.9872 - val_loss: 0.2415 - val_accuracy: 0.9540\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 45s 97ms/step - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.2212 - val_accuracy: 0.9627\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.0251 - accuracy: 0.9922 - val_loss: 0.1911 - val_accuracy: 0.9637\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 46s 97ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.2287 - val_accuracy: 0.9537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ec2cd8850>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11d5a9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Bean', 1: 'Bitter_Gourd', 2: 'Bottle_Gourd', 3: 'Brinjal', 4: 'Broccoli', 5: 'Cabbage', 6: 'Capsicum', 7: 'Carrot', 8: 'Cauliflower', 9: 'Cucumber', 10: 'Papaya', 11: 'Potato', 12: 'Pumpkin', 13: 'Radish', 14: 'Tomato'}\n"
     ]
    }
   ],
   "source": [
    "class_map = dict([(v,k) for k,v in train_generator.class_indices.items()])\n",
    "print(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5265cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capsicum\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'C:\\Users\\mayan\\Downloads\\archive (6)\\Vegetable Images\\validation\\\\Capsicum\\1202.jpg', target_size = (64, 64)) # location of image\n",
    "test_image = image.img_to_array(test_image)    # converting to array\n",
    "test_image = np.expand_dims(test_image, axis = 0)  # selecting dimension 1 and batch_size\n",
    "result = cnn.predict(test_image)    \n",
    "\n",
    "if result[0][0] == 1:  \n",
    "    print('Bean')\n",
    "elif result[0][1] == 1.0:  \n",
    "    print('Bitter_Gourd')\n",
    "elif result[0][2] == 1.0:  \n",
    "    print('Bottle_Gourd')\n",
    "elif result[0][3] == 1.0:  \n",
    "    print('Brinjal')\n",
    "elif result[0][4] == 1.0:  \n",
    "    print('Broccoli')\n",
    "elif result[0][5] == 1.0:  \n",
    "    print('Cabbage')\n",
    "elif result[0][6] == 1.0:  \n",
    "    print('Capsicum')\n",
    "elif result[0][7] == 1.0:  \n",
    "    print('Carrot')\n",
    "elif result[0][8] == 1.0:  \n",
    "    print('Cauliflower')\n",
    "elif result[0][9] == 1.0:  \n",
    "    print('Cucumber')\n",
    "elif result[0][10] == 1.0:  \n",
    "    print('Papaya')\n",
    "elif result[0][11] == 1.0:  \n",
    "    print('Potato')\n",
    "elif result[0][12] == 1.0:  \n",
    "    print('Pumpkin')\n",
    "elif result[0][13] == 1.0:  \n",
    "    print('Radish')\n",
    "elif result[0][14] == 1.0:  \n",
    "    print('Tomato')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bade295e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bean': 0,\n",
       " 'Bitter_Gourd': 1,\n",
       " 'Bottle_Gourd': 2,\n",
       " 'Brinjal': 3,\n",
       " 'Broccoli': 4,\n",
       " 'Cabbage': 5,\n",
       " 'Capsicum': 6,\n",
       " 'Carrot': 7,\n",
       " 'Cauliflower': 8,\n",
       " 'Cucumber': 9,\n",
       " 'Papaya': 10,\n",
       " 'Potato': 11,\n",
       " 'Pumpkin': 12,\n",
       " 'Radish': 13,\n",
       " 'Tomato': 14}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48be85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "94350dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "622cbd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_model():\n",
    "    model = load_model('cnn.h5')\n",
    "    return model\n",
    "model = load_model()\n",
    "st.write(\"\"\"\n",
    "          # Vegetable Prediction\n",
    "         \"\"\"\n",
    "        )\n",
    "file = st.file_uploader(\"Please upload an vegetable image\",type=['jpg'])\n",
    "import cv2\n",
    "from PIL import Image,ImageOps\n",
    "import numpy as np\n",
    "def import_and_predict(image_data,model):\n",
    "    \n",
    "    size = (64,64)\n",
    "    image = ImageOps,fit(image_data,size,Image.ANTIALIAS)\n",
    "    img = np.asarray(image)\n",
    "    img_reshape = img[np.newaxis,...]\n",
    "    prediction = model.predict(img_reshape)\n",
    "    \n",
    "    return prediction\n",
    "if file is None:\n",
    "    st.text(\"Please Upload an image file\")\n",
    "else:\n",
    "    image = Image.open(file)\n",
    "    st.image(image,use_column,width=True)\n",
    "    predictions = import_and_predict(image,model)\n",
    "    class_names = ['Bean','Bitter_Gourd','Bottle_Gourd','Brinjal','Broccoli','Cabbage','Capsicum','Carrot','Cauliflower','Cucumber','Papaya','Potato','Pumpkin','Radish','Tomato']\n",
    "    string = \"This image most likely is : \"+ class_names[np.argmax(predictions)]\n",
    "    st.success(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "913acefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47b2c0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17ec4e002e0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd88d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
