{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895ab2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from keras.layers import Convolution2D ,Activation, Conv2D,MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff96b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'C:\\Users\\mayan\\Downloads\\archive (6)\\Vegetable Images\\train'\n",
    "test_dir = r'C:\\Users\\mayan\\Downloads\\archive (6)\\Vegetable Images\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9d2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,    # use for feature scaling\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)    # image transformation to avoid overfitting\n",
    "training_set = train_datagen.flow_from_directory(train_dir,   # directory path\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d781ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)   # only rescaling for test set like previous where we apply only transform\n",
    "test_set = test_datagen.flow_from_directory(test_dir,   # method to test set and fit_transform to training set\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89af7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da4e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e6df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f117e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1aab284",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "#keep same second convolution layer except the input_size....we dont need another input to put in the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f56033",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd1b0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e3d1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ffb8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=256, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c504cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdddf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=512, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3c2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f634d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=15, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3efeadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec14bb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 235s 497ms/step - loss: 1.7804 - accuracy: 0.3781 - val_loss: 1.1269 - val_accuracy: 0.5913\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 87s 185ms/step - loss: 1.0455 - accuracy: 0.6437 - val_loss: 1.2088 - val_accuracy: 0.6447\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 87s 185ms/step - loss: 0.7509 - accuracy: 0.7540 - val_loss: 0.5777 - val_accuracy: 0.8153\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 84s 179ms/step - loss: 0.5955 - accuracy: 0.8095 - val_loss: 0.3969 - val_accuracy: 0.8730\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 84s 180ms/step - loss: 0.4861 - accuracy: 0.8460 - val_loss: 0.5310 - val_accuracy: 0.8450\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 85s 180ms/step - loss: 0.4236 - accuracy: 0.8689 - val_loss: 0.3319 - val_accuracy: 0.8967\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 85s 181ms/step - loss: 0.3825 - accuracy: 0.8819 - val_loss: 0.4329 - val_accuracy: 0.8683\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.3525 - accuracy: 0.8890 - val_loss: 0.3205 - val_accuracy: 0.9003\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 45s 96ms/step - loss: 0.3264 - accuracy: 0.8990 - val_loss: 0.3276 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 44s 94ms/step - loss: 0.2999 - accuracy: 0.9093 - val_loss: 0.2440 - val_accuracy: 0.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221ea7cff40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5265cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capsicum\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'C:\\Users\\mayan\\Downloads\\archive (6)\\Vegetable Images\\validation\\\\Capsicum\\1202.jpg', target_size = (64, 64)) # location of image\n",
    "test_image = image.img_to_array(test_image)    # converting to array\n",
    "test_image = np.expand_dims(test_image, axis = 0)  # selecting dimension 1 and batch_size\n",
    "result = cnn.predict(test_image)\n",
    "class_names = ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Capsicum', 'Carrot',\n",
    "                   'Cauliflower', 'Cucumber', 'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']\n",
    "print(class_names[np.argmax(result)])\n",
    "# if result[0][0] == 1:  \n",
    "#     print('Bean')\n",
    "# elif result[0][1] == 1.0:  \n",
    "#     print('Bitter_Gourd')\n",
    "# elif result[0][2] == 1.0:  \n",
    "#     print('Bottle_Gourd')\n",
    "# elif result[0][3] == 1.0:  \n",
    "#     print('Brinjal')\n",
    "# elif result[0][4] == 1.0:  \n",
    "#     print('Broccoli')\n",
    "# elif result[0][5] == 1.0:  \n",
    "#     print('Cabbage')\n",
    "# elif result[0][6] == 1.0:  \n",
    "#     print('Capsicum')\n",
    "# elif result[0][7] == 1.0:  \n",
    "#     print('Carrot')\n",
    "# elif result[0][8] == 1.0:  \n",
    "#     print('Cauliflower')\n",
    "# elif result[0][9] == 1.0:  \n",
    "#     print('Cucumber')\n",
    "# elif result[0][10] == 1.0:  \n",
    "#     print('Papaya')\n",
    "# elif result[0][11] == 1.0:  \n",
    "#     print('Potato')\n",
    "# elif result[0][12] == 1.0:  \n",
    "#     print('Pumpkin')\n",
    "# elif result[0][13] == 1.0:  \n",
    "#     print('Radish')\n",
    "# elif result[0][14] == 1.0:  \n",
    "#     print('Tomato')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48be85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94350dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622cbd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def load_model():\n",
    "    model = load_model('cnn.h5')\n",
    "    return model\n",
    "model = load_model()\n",
    "st.write(\"\"\"\n",
    "          # Vegetable Prediction\n",
    "         \"\"\"\n",
    "        )\n",
    "file = st.file_uploader(\"Please upload an vegetable image\",type=['jpg'])\n",
    "import cv2\n",
    "from PIL import Image,ImageOps\n",
    "import numpy as np\n",
    "def import_and_predict(image_data,model):\n",
    "    \n",
    "    size = (64,64)\n",
    "    image = ImageOps,fit(image_data,size,Image.ANTIALIAS)\n",
    "    img = np.asarray(image)\n",
    "    img_reshape = img[np.newaxis,...]\n",
    "    prediction = model.predict(img_reshape)\n",
    "    \n",
    "    return prediction\n",
    "if file is None:\n",
    "    st.text(\"Please Upload an image file\")\n",
    "else:\n",
    "    image = Image.open(file)\n",
    "    st.image(image,use_column,width=True)\n",
    "    predictions = import_and_predict(image,model)\n",
    "    class_names = ['Bean','Bitter_Gourd','Bottle_Gourd','Brinjal','Broccoli','Cabbage','Capsicum','Carrot','Cauliflower','Cucumber','Papaya','Potato','Pumpkin','Radish','Tomato']\n",
    "    string = \"This image most likely is : \"+ class_names[np.argmax(predictions)]\n",
    "    st.success(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913acefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2c0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd88d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
